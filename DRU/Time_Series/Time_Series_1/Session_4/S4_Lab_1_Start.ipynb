{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datarobot as dr\n",
    "from datarobot import Project, Deployment\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.ticker as mtick\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "import dateutil.parser\n",
    "import os\n",
    "import re \n",
    "from importlib import reload\n",
    "import random\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Set Pandas configuration to show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr.Client(config_path='../drconfig.yaml');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"\"\" Enter Code \"\"\")\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHS = '' \n",
    "\n",
    "def months(df):\n",
    "    global MONTHS\n",
    "    MIN_DATE = df['Date'].min()\n",
    "    MAX_DATE = df['Date'].max()\n",
    "    MONTHS = str(int((MAX_DATE - MIN_DATE).days / 30))\n",
    "     \n",
    "    print('Min Date: ', MIN_DATE)\n",
    "    print('Max Date: ', MAX_DATE)\n",
    "    print('Months:   ', MONTHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TS Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE      = \"\"\" Enter Code \"\"\"\n",
    "TARGET    = \"\"\" Enter Code \"\"\"\n",
    "\n",
    "PROJECT_NAME = 'Lab_4'\n",
    "\n",
    "VERSION = '1'\n",
    "MODE    = 'Q'\n",
    "\n",
    "FDWS = \"\"\" Enter Code \"\"\" \n",
    "\n",
    "FDS  = \"\"\" Enter Code \"\"\" \n",
    "\n",
    "BASE   = 'L4_1_V:'\n",
    "PREFIX = BASE + VERSION + '_Mnths:' + str(MONTHS) + '_Mode:' + MODE\n",
    "DATASET_FILENAME = 'Months_' + str(MONTHS)\n",
    "MAX_WAIT = 14400\n",
    "READ_TIMEOUT = 14400\n",
    "\n",
    "HOLDOUT_START_DATE  = None \n",
    "VALIDATION_DURATION = None \n",
    "HOLDOUT_DURATION    = None \n",
    "NUMBER_BACKTESTS    = None\n",
    "GAP_DURATION        = None \n",
    "\n",
    "FEATURE_SETTINGS = []\n",
    "\n",
    "CAL_ID = None\n",
    "\n",
    "print(FEATURE_SETTINGS)\n",
    "print(CAL_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dr_project(df, project_name, fw_start=None, fw_end=None, fdw_start=None, fdw_end=None, dataset_filename=DATASET_FILENAME):\n",
    "    ###############################\n",
    "    # Create Datetime Specification\n",
    "    ###############################\n",
    "    # SERIES_COL = [SERIES]\n",
    "    time_partition = dr.DatetimePartitioningSpecification(\n",
    "        datetime_partition_column = DATE,\n",
    "        forecast_window_start     = fw_start, \n",
    "        forecast_window_end       = fw_end,\n",
    "        feature_derivation_window_start = fdw_start,\n",
    "        feature_derivation_window_end   = fdw_end,\n",
    "        holdout_start_date        = HOLDOUT_START_DATE ,\n",
    "        validation_duration       = VALIDATION_DURATION,  \n",
    "        holdout_duration          = HOLDOUT_DURATION,\n",
    "        number_of_backtests       = NUMBER_BACKTESTS, \n",
    "        feature_settings          = FEATURE_SETTINGS,\n",
    "        use_time_series           = True\n",
    "      )\n",
    "     \n",
    "\n",
    "    ################\n",
    "    # Create Project\n",
    "    ################\n",
    "    project = dr.Project.create(\n",
    "        project_name = project_name, \n",
    "        sourcedata   = df, \n",
    "        max_wait     = MAX_WAIT, \n",
    "        read_timeout = READ_TIMEOUT,\n",
    "        dataset_filename = DATASET_FILENAME\n",
    "    )\n",
    "    print(\"Post-Project MB: \", (df.memory_usage(index=True).sum()/1024/1024).round(2))\n",
    "    print(\"Post-Project Records: {:,}\".format(len(df)))\n",
    "    print(f'Project {project_name} Created...')\n",
    "    print(\" \")\n",
    "\n",
    "    #################\n",
    "    # Start Autopilot\n",
    "    #################\n",
    "    project.set_target(\n",
    "        target = TARGET,   \n",
    "        metric = None,      \n",
    "        mode   = dr.AUTOPILOT_MODE.QUICK , # dr.AUTOPILOT_MODE.FULL_AUTO,\n",
    "        #advanced_options = opts,\n",
    "        worker_count = -1,\n",
    "        partitioning_method = time_partition,\n",
    "        max_wait = MAX_WAIT\n",
    "    )\n",
    "    return project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = []  # Keep List of all project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_factory(df, FDWS, FDS):\n",
    "    PREFIX = BASE + str(VERSION) + '_Mnths:' + MONTHS + '_Mode:' + MODE\n",
    "    DATASET_FILENAME = 'Months_' + MONTHS\n",
    "    result = len(FDWS) * len(FDS)\n",
    "    proj_num = 1\n",
    "    print(f\"Kicking off {result} projects!\")\n",
    "    print(\" \")\n",
    "    \n",
    "    \n",
    "    for fdw in FDWS:\n",
    "        for fd in FDS:\n",
    "            fd_start  = fd[0] \n",
    "            fd_end    = fd[1]\n",
    "            fdw_start = fdw[0]\n",
    "            fdw_end   = fdw[1]\n",
    "            \n",
    "            print(f\"Project {proj_num}\")\n",
    "\n",
    "            # Name project\n",
    "            project_name = f\"{PREFIX}_FDW:{fdw_start}-{fdw_end}_FD:{fd_start}-{fd_end}\"  \n",
    "            print(project_name)\n",
    "            print(\" \")\n",
    "\n",
    "            data = df.copy() \n",
    "\n",
    "            # Create project\n",
    "            project = create_dr_project(data, project_name, \n",
    "                                        fw_start=fd_start, fw_end=fd_end, \n",
    "                                        fdw_start=fdw_start, fdw_end=fdw_end,\n",
    "                                        dataset_filename=DATASET_FILENAME)\n",
    "\n",
    "            projects.append(project) \n",
    "            proj_num = proj_num + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_factory(df, FDWS, FDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over various FDWs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FDWS = \"\"\" Enter Code \"\"\"  \n",
    "\n",
    "FDS  = \"\"\" Enter Code \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_factory(df, FDWS, FDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Project Names in a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = dr.Project.list(search_params={'project_name': BASE}) \n",
    "projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Project Names and PIDs in a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "\n",
    "for p in projects:\n",
    "    r = ((p, p.id))\n",
    "    lst.append(r)\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unlock Holdouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lst:\n",
    "    project = Project.get(i[1])\n",
    "    project.unlock_holdout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Backtests for Blenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lst:\n",
    "    project = Project.get(i[1])\n",
    "    lb = project.get_datetime_models()\n",
    "    for model in lb:\n",
    "        \n",
    "        if 'Blender' in model.model_type:\n",
    "            try:\n",
    "                print(project.project_name, model)  # , model.id\n",
    "                dr.DatetimeModel.score_backtests(model) \n",
    "                print(f'Computing backtests for model {model.id} in Project {project.project_name}')\n",
    "            except dr.errors.ClientError:\n",
    "                pass\n",
    "            print(f'All available backtests have been submitted for scoring for project {project.project_name}')\n",
    "            print(' ')\n",
    "        else:\n",
    "            None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute All Backtests for Top Models in Backtest 1 and Holdout groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZE_GROUP = ['validation', 'holdout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_METRIC = project.metric\n",
    "METRICS = list(set([PROJECT_METRIC, 'MASE', 'RMSE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in lst :\n",
    "    for met in METRICS:\n",
    "        for o in OPTIMIZE_GROUP:\n",
    "            project = Project.get(p[1])\n",
    "            lb = project.get_datetime_models()\n",
    "\n",
    "            best_models = sorted(\n",
    "                                [model for model in lb if model.metrics[met][o]],  \n",
    "                                key=lambda m: m.metrics[met][o],\n",
    "                                )[0:3]\n",
    "            \n",
    "            for mod in best_models:\n",
    "\n",
    "                if mod.metrics[met][\"backtesting\"] == None:\n",
    "                    try:\n",
    "                        print(project.project_name, mod)  \n",
    "                        dr.DatetimeModel.score_backtests(mod) \n",
    "                        print(f'Computing backtests for model {mod.model_type} in Project {project.project_name}')\n",
    "                    except dr.errors.ClientError:\n",
    "                        pass\n",
    "                    print(f'All available backtests have been submitted for scoring for project {project.project_name}')\n",
    "                    print(' ')\n",
    "                else:\n",
    "                    print(project.project_name)\n",
    "                    print(f'{mod.model_type} All Backtests Already Computed')\n",
    "                    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Project and Model Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter correct partition period\n",
    "OPTIMIZATION_PERIOD = \"\"\" Enter Code \"\"\"  # BackTest 1: validation  All Backtest: backtesting  Holdout: holdout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "scores = pd.DataFrame()\n",
    "\n",
    "\n",
    "for p in lst:\n",
    "    project = Project.get(p[1])\n",
    "    lb = project.get_datetime_models()\n",
    "    best_model = sorted(\n",
    "                        [model for model in lb if model.metrics[project.metric][OPTIMIZATION_PERIOD]],  \n",
    "                        key=lambda m: m.metrics[project.metric][OPTIMIZATION_PERIOD],\n",
    "                        )[0]\n",
    "\n",
    "    backtest_scores = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                'Project_Name': project.project_name,\n",
    "                'Project_ID': project.id,\n",
    "                'Model_ID': best_model.id,\n",
    "                'Model_Type': best_model.model_type,\n",
    "                'Featurelist': best_model.featurelist_name,\n",
    "                'Optimization_Metric': project.metric,\n",
    "                'Scores': best_model.metrics,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    scores = scores.append(backtest_scores, sort=False).reset_index(drop=True)  \n",
    "\n",
    "\n",
    "print(f'Scores for all {len(projects)} projects have been computed')\n",
    "print('')\n",
    "\n",
    "scores = scores.join(json_normalize(scores[\"Scores\"].tolist())).drop(labels=['Scores'], axis=1) \n",
    "\n",
    "# Drop Empty Columns\n",
    "scores = scores[scores.columns.drop(list(scores.filter(regex='crossValidation$')))]\n",
    "\n",
    "# Rename Columns\n",
    "scores.columns = scores.columns.str.replace(\".backtesting\", \"_All_BT\")\n",
    "scores.columns = scores.columns.str.replace(\".holdout\", \"_Holdout\")\n",
    "scores.columns = scores.columns.str.replace(\".validation\", \"_BT_1\")\n",
    "scores.columns = scores.columns.str.replace(' ', '_')\n",
    "\n",
    "scores = scores[scores.columns.drop(list(scores.filter(regex='_All_BTScores$')))]\n",
    "\n",
    "scores.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = scores.filter(regex='MASE|RMSE').columns.to_list()\n",
    "PROJECT = ['Project_Name', 'Project_ID', 'Model_ID', 'Model_Type', 'Featurelist']\n",
    "COLS = PROJECT + METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['FDW_Start'] = scores['Project_Name'].str.extract(r'FDW:(-\\d{1,2})')\n",
    "scores['FDW_End']   = scores['Project_Name'].str.extract(r'FDW:-\\d{1,2}-(\\d{1,2})_')\n",
    "scores['FD_Start']  = scores['Project_Name'].str.extract(r'FD:(\\d{1,2})')\n",
    "scores['FD_End']    = scores['Project_Name'].str.extract(r'FD:\\d{1,2}-(\\d{1,2})')\n",
    "scores['Months']    = scores['Project_Name'].str.extract(r'_Mnths:(\\d{1,2})_')\n",
    "\n",
    "scores.rename(columns={'All_Backtests_Poisson Deviance':'All_Backtests_Poisson_Deviance', \n",
    "                       'Backtest_1_Poisson Deviance':'Backtest_1_Poisson_Deviance',\n",
    "                       'Holdout_Poisson Deviance':'Holdout_Poisson_Deviance',\n",
    "                       'Holdout_Tweedie Deviance':'Holdout_Tweedie_Deviance',\n",
    "                       'All_Backtests_Tweedie Deviance':'All_Backtests_Tweedie_Deviance',\n",
    "                       'Backtest_1_Tweedie Deviance':'Backtest_1_Tweedie_Deviance',\n",
    "                       'Holdout_Tweedie Deviance':'Holdout_Tweedie_Deviance'}, inplace=True)\n",
    "\n",
    "\n",
    "META = ['FDW_Start', 'FDW_End', 'FD_Start', 'FD_End', 'Months']\n",
    "MORE = PROJECT + META + METRICS \n",
    "  \n",
    "scores[MORE].sort_values(by=['MASE_All_BT'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[MORE].sort_values(by=['MASE_All_BT'], ascending=True).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter column with correct scoring metric and partition\n",
    "hrmse = scores.loc[scores[\"\"\" Enter Code \"\"\"].notnull()]\n",
    "\n",
    "# Take the Single Best model\n",
    "hrmse_best = pd.DataFrame(hrmse.loc[hrmse.MASE_All_BT.idxmin()]).transpose()\n",
    "\n",
    "# Take the Best model by Project Name\n",
    "# hrmse_best = hrmse.loc[hrmse.groupby('Project_Name').MASE_All_BT.idxmin()]\n",
    "\n",
    "best_models = pd.DataFrame(hrmse_best) \n",
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Feature Impact from Top Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECORD = \"\"\" Enter Code \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify correct model\n",
    "PID = best_models['Project_ID'].values[RECORD]\n",
    "MID = best_models['Model_ID'].values[RECORD]\n",
    "\n",
    "project = dr.Project.get(PID)\n",
    "model   = dr.Model.get(PID, MID)\n",
    "print(project)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PID = best_models['Project_ID'].values[RECORD]\n",
    "MID = best_models['Model_ID'].values[RECORD]\n",
    "\n",
    "project = dr.Project.get(PID)\n",
    "print(project)\n",
    "print(\" \")\n",
    "\n",
    "model   = dr.Model.get(PID, MID)\n",
    "print(model)\n",
    "print(\" \")\n",
    "\n",
    "feature_impacts = model.get_or_request_feature_impact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_dark_blue = '#08233F'\n",
    "dr_blue      = '#1F77B4'\n",
    "dr_orange    = '#FF7F0E'\n",
    "dr_red       = '#BE3C28'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_tick_fmt = mtick.PercentFormatter(xmax=1.0)\n",
    "\n",
    "impact_df = pd.DataFrame(feature_impacts)\n",
    "impact_df.sort_values(by='impactNormalized', ascending=True, inplace=True)\n",
    "\n",
    "# Positive values are blue, negative are red\n",
    "bar_colors = impact_df.impactNormalized.apply(lambda x: dr_red if x < 0\n",
    "                                              else dr_blue)\n",
    "\n",
    "ax = impact_df.plot.barh(x='featureName', y='impactNormalized',\n",
    "                         legend=False,\n",
    "                         color=bar_colors,\n",
    "                         figsize=(12, 14))\n",
    "ax.xaxis.set_major_formatter(percent_tick_fmt)\n",
    "ax.xaxis.set_tick_params(labeltop=True)\n",
    "ax.xaxis.grid(True, alpha=0.2)\n",
    "ax.set_facecolor(dr_dark_blue)\n",
    "\n",
    "plt.ylabel('')\n",
    "plt.xlabel('Normalized Impact')\n",
    "plt.xlim((None, 1))  # Allow for negative impact\n",
    "plt.title('Feature Impact', y=1.04);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matplotlib_pair_histogram(labels, counts, target_avgs,\n",
    "                              bin_count, ax1, feature):\n",
    "    \n",
    "#     ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "#     ax.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "    \n",
    "    # Rotate categorical labels\n",
    "    if feature.feature_type in ['Categorical', 'Text', 'Numeric']:\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "    ax1.set_ylabel(feature.name, color=dr_blue)\n",
    "    ax1.bar(labels, counts, color=dr_blue)\n",
    "    ax1.set_xticklabels([str(round(float(label), 2)) for label in labels])\n",
    "    \n",
    "    # Instantiate a second axes that shares the same x-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel(TARGET, color=dr_orange)\n",
    "    ax2.plot(labels, target_avgs, marker='o', lw=1, color=dr_orange)\n",
    "    ax1.set_facecolor(dr_dark_blue)\n",
    "    title = 'Histogram for {} ({} bins)'.format(feature.name, bin_count)\n",
    "    ax1.set_title(title)\n",
    "    \n",
    "def draw_feature_histogram(feature_name, bin_count):\n",
    "    feature = dr.Feature.get(project.id, feature_name)\n",
    "    # Retrieve downsampled histogram data\n",
    "    # Based on desired bin count\n",
    "    data = feature.get_histogram(bin_count).plot\n",
    "    \n",
    "    data = pd.DataFrame(data, columns=['label', 'count', 'target'])\n",
    "    data['label'] = data['label'].astype(float).astype(int).astype(str)\n",
    "    data = data.to_dict(orient='records')\n",
    "    \n",
    "    labels = [row['label'] for row in data]\n",
    "    counts = [row['count'] for row in data]\n",
    "    target_averages = [row['target'] for row in data]\n",
    "    f, axarr = plt.subplots()\n",
    "    f.set_size_inches((10, 4))\n",
    "    matplotlib_pair_histogram(labels, counts, target_averages,\n",
    "                              bin_count, axarr, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.get_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_feature_histogram('retail_sales', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduced Feature Lists\n",
    "<font color=lightblue>\n",
    "Three Methods to Choose from:</font> <br>\n",
    "&nbsp;&nbsp; &nbsp; &nbsp; 1. Percent of Top Features <br> \n",
    "&nbsp;&nbsp; &nbsp; &nbsp; 2. Number of Top Features <br>\n",
    "&nbsp;&nbsp; &nbsp; &nbsp; 3. Manually Specifying Features <br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame to store Feature List Names and IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_lists_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top X Percent of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter Threshold Percentage\n",
    "THRESHOLD = \"\"\" Enter Code \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Collecting Feature Impact for M{model.model_number} in project \"{project.project_name}\"')\n",
    "\t\n",
    "impact = pd.DataFrame.from_records(model.get_or_request_feature_impact())\n",
    "impact['cumulative_impact'] = impact['impactUnnormalized'].cumsum() / impact['impactUnnormalized'].sum()\n",
    "\n",
    "to_keep = np.where(impact['cumulative_impact'] <= THRESHOLD)[0]\n",
    "if len(to_keep) < 1:\n",
    "    print('Applying this threshold would result in a featurelist with no features')\n",
    "\n",
    "\n",
    "idx = np.max(to_keep)\n",
    "\n",
    "selected_features = impact.loc[0:idx, 'featureName'].to_list()\n",
    "feature_list = project.create_modeling_featurelist(f'Top {len(selected_features)} features M{model.model_number}', \n",
    "                                                   selected_features)\n",
    "\n",
    "\n",
    "cust_feat_list = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            'Name': feature_list.name,\n",
    "            'ID': feature_list.id\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "feature_lists_df = feature_lists_df.append(cust_feat_list, sort=False).reset_index(drop=True) \n",
    "\n",
    "feature_lists_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = \"\"\" Enter Code \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Collecting Feature Impact for M{model.model_number} in project \"{project.project_name}\"')\n",
    "\t\n",
    "impact = pd.DataFrame.from_records(model.get_or_request_feature_impact())\n",
    "impact['cumulative_impact'] = impact['impactUnnormalized'].cumsum() / impact['impactUnnormalized'].sum()\n",
    "\n",
    "to_keep = np.where(impact['cumulative_impact'] <= THRESHOLD)[0]\n",
    "if len(to_keep) < 1:\n",
    "    print('Applying this threshold would result in a featurelist with no features')\n",
    "\n",
    "\n",
    "idx = np.max(to_keep)\n",
    "\n",
    "selected_features = impact.loc[0:idx, 'featureName'].to_list()\n",
    "feature_list = project.create_modeling_featurelist(f'Top {len(selected_features)} features M{model.model_number}', \n",
    "                                                   selected_features)\n",
    "\n",
    "\n",
    "cust_feat_list = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            'Name': feature_list.name,\n",
    "            'ID': feature_list.id\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "feature_lists_df = feature_lists_df.append(cust_feat_list, sort=False).reset_index(drop=True) \n",
    "\n",
    "feature_lists_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top X Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter Number of features to include\n",
    "MAX_FEATURES = \"\"\" Enter Code \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Collecting Feature Impact for M{model.model_number} in project \"{project.project_name}\"')\n",
    "\n",
    "impact = model.get_or_request_feature_impact()\n",
    "\n",
    "impact.sort(key=lambda x: x['impactNormalized'], reverse=True)\n",
    "feature_list_items = [f['featureName'] for f in impact[:MAX_FEATURES]]\n",
    "\n",
    "feature_list = project.create_modeling_featurelist(f'Top {MAX_FEATURES} features M{model.model_number}', \n",
    "                                                   feature_list_items)\n",
    "\n",
    "\n",
    "cust_feat_list = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            'Name': feature_list.name,\n",
    "            'ID': feature_list.id\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "feature_lists_df = feature_lists_df.append(cust_feat_list, sort=False).reset_index(drop=True) \n",
    "\n",
    "feature_lists_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Select Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.get_modeling_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Feature By Normalized Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_impacts = model.get_or_request_feature_impact()\n",
    "\n",
    "feature_impacts.sort(key=lambda x: x['impactNormalized'], reverse=True)\n",
    "feature_impacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Feature List 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features from list above\n",
    "FEATURES = [\"\"\" Enter Code \"\"\"]\n",
    "\n",
    "print(len(FEATURES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = project.create_modeling_featurelist(f'Manual Feature Selection {len(FEATURES)}', FEATURES)\n",
    "\n",
    "\n",
    "cust_feat_list = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            'Name': feature_list.name,\n",
    "            'ID': feature_list.id\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "feature_lists_df = feature_lists_df.append(cust_feat_list, sort=False).reset_index(drop=True) \n",
    "\n",
    "feature_lists_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Feature List 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [\"\"\" Enter Code \"\"\"]\n",
    "\n",
    "print(len(FEATURES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = project.create_modeling_featurelist(f'Manual Feature Selection {len(FEATURES)}', FEATURES)\n",
    "\n",
    "cust_feat_list = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            'Name': feature_list.name,\n",
    "            'ID': feature_list.id\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "feature_lists_df = feature_lists_df.append(cust_feat_list, sort=False).reset_index(drop=True) \n",
    "\n",
    "feature_lists_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List all Feature Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.get_modeling_featurelists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Blueprints on new Feature Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_lists_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Models/Blueprints to run new Feature Lists on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_lb = pd.DataFrame()\n",
    "\n",
    "\n",
    "lb = project.get_datetime_models()\n",
    "best_models = sorted(\n",
    "                    [model for model in lb if model.metrics[project.metric][OPTIMIZATION_PERIOD]],  \n",
    "                    key=lambda m: m.metrics[project.metric][OPTIMIZATION_PERIOD],\n",
    "                    )\n",
    "\n",
    "for m in best_models:\n",
    "    backtest_scores = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "#                 'Project_Name': project.project_name,\n",
    "                'Project_ID': project.id,\n",
    "                'Model_ID': m.id,\n",
    "                'Model_Type': m.model_type,\n",
    "                'Featurelist': m.featurelist_name,\n",
    "                'Optimization_Metric': project.metric,\n",
    "                'Scores': m.metrics,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    scores_lb = scores_lb.append(backtest_scores, sort=False).reset_index(drop=True)  \n",
    "\n",
    "scores_lb = scores_lb.join(json_normalize(scores_lb[\"Scores\"].tolist())).drop(labels=['Scores'], axis=1) \n",
    "\n",
    "# Drop Empty Columns\n",
    "scores_lb = scores_lb[scores_lb.columns.drop(list(scores.filter(regex='crossValidation$')))]\n",
    "\n",
    "# Rename Columns\n",
    "scores_lb.columns = scores_lb.columns.str.replace(\".backtesting\", \"_All_BT\")\n",
    "scores_lb.columns = scores_lb.columns.str.replace(\".holdout\", \"_Holdout\")\n",
    "scores_lb.columns = scores_lb.columns.str.replace(\".validation\", \"_BT_1\")\n",
    "scores_lb.columns = scores_lb.columns.str.replace(' ', '_')\n",
    "\n",
    "scores_lb = scores_lb[scores_lb.columns.drop(list(scores_lb.filter(regex='_All_BTScores$')))]\n",
    "\n",
    "scores_lb.sort_values(by=[\"\"\" Enter Code \"\"\"], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter Model ID's to run\n",
    "MODEL_LIST = [\"\"\" Enter Code \"\"\", \"\"\" Enter Code \"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run New Feature Lists against selected Blueprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DURATION = dr.helpers.partitioning_methods.construct_duration_string(years=17, months=5, days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PID = scores_lb['Project_ID'][0]\n",
    "\n",
    "for m in MODEL_LIST :\n",
    "    model = dr.Model.get(PID, m)\n",
    "    \n",
    "    for fl in feature_lists_df.values:\n",
    "        fl_id = fl[1] \n",
    "        try:\n",
    "            model.train_datetime(featurelist_id = fl_id, \n",
    "                                 training_duration = DURATION)\n",
    "            print(f\"Running Feature List {fl[0]} on Model {model.model_type}\")\n",
    "        except dr.errors.ClientError:\n",
    "            pass\n",
    "        print(f\"Feature List {fl[0]} already run on Model {model.model_type}\")\n",
    "        print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Backtests for Blueprints run on new Feature Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZE_GROUP = ['validation', 'holdout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_METRIC = project.metric\n",
    "METRICS = list(set([PROJECT_METRIC, 'MASE', 'RMSE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for met in METRICS:\n",
    "    for o in OPTIMIZE_GROUP:\n",
    "        project = Project.get(PID)\n",
    "        lb = project.get_datetime_models()\n",
    "\n",
    "        best_models = sorted(\n",
    "                            [model for model in lb if model.metrics[met][o]],  \n",
    "                            key=lambda m: m.metrics[met][o],\n",
    "                            )[0:5]\n",
    "\n",
    "        for mod in best_models:\n",
    "\n",
    "            if mod.metrics[met][\"backtesting\"] == None:\n",
    "                try:\n",
    "                    print(project.project_name, mod)  \n",
    "                    dr.DatetimeModel.score_backtests(mod) \n",
    "                    print(f'Computing backtests for model {mod.model_type} in Project {project.project_name}')\n",
    "                except dr.errors.ClientError:\n",
    "                    pass\n",
    "                print(f'All available backtests have been submitted for scoring for project {project.project_name}')\n",
    "                print(' ')\n",
    "            else:\n",
    "                print(project.project_name)\n",
    "                print(f'{mod.model_type} All Backtests Already Computed')\n",
    "                print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Project and Model Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter correct optimization period\n",
    "OPTIMIZATION_PERIOD = \"\"\" Enter Code \"\"\"  # BackTest 1: validation  All Backtest: backtesting  Holdout: holdout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "scores = pd.DataFrame()\n",
    "\n",
    "\n",
    "project = Project.get(PID)\n",
    "lb = project.get_datetime_models()\n",
    "best_model = sorted(\n",
    "                    [model for model in lb if model.metrics[project.metric][OPTIMIZATION_PERIOD]],  \n",
    "                    key=lambda m: m.metrics[project.metric][OPTIMIZATION_PERIOD],\n",
    "                    )[:]\n",
    "\n",
    "for m in best_model:\n",
    "    backtest_scores = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                'Project_Name': project.project_name,\n",
    "                'Project_ID': project.id,\n",
    "                'Model_ID': m.id,\n",
    "                'Model_Type': m.model_type,\n",
    "                'Featurelist': m.featurelist_name,\n",
    "                'Optimization_Metric': project.metric,\n",
    "                'Scores': m.metrics,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    scores = scores.append(backtest_scores, sort=False).reset_index(drop=True)  \n",
    "\n",
    "\n",
    "print(f'Scores for all {len(projects)} projects have been computed')\n",
    "print('')\n",
    "\n",
    "scores = scores.join(json_normalize(scores[\"Scores\"].tolist())).drop(labels=['Scores'], axis=1) \n",
    "\n",
    "# Drop Empty Columns\n",
    "scores = scores[scores.columns.drop(list(scores.filter(regex='crossValidation$')))]\n",
    "\n",
    "# Rename Columns\n",
    "scores.columns = scores.columns.str.replace(\".backtesting\", \"_All_BT\")\n",
    "scores.columns = scores.columns.str.replace(\".holdout\", \"_Holdout\")\n",
    "scores.columns = scores.columns.str.replace(\".validation\", \"_BT_1\")\n",
    "scores.columns = scores.columns.str.replace(' ', '_')\n",
    "\n",
    "scores = scores[scores.columns.drop(list(scores.filter(regex='_All_BTScores$')))]\n",
    "\n",
    "scores.sort_values(by=['MASE_All_BT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.sort_values(by=['MASE_All_BT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Numeric to Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project.create_type_transform_feature(\n",
    "#         \"retail_sales(Cat)\",  # new feature name\n",
    "#         \"retail_sales\",       # parent name\n",
    "#         dr.enums.VARIABLE_TYPE_TRANSFORM.CATEGORICAL_INT\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Categorical to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project.create_type_transform_feature(\n",
    "#     \"addr_state(Text)\",  # new feature name\n",
    "#     \"addr_state\",        # parent name\n",
    "#     dr.enums.VARIABLE_TYPE_TRANSFORM.TEXT\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
